---
layout: post
title: "Best Practice Configs"
tags: [Fabric, Spark, Lakehouse, Delta Lake, DuckDB, Polars, Daft]
categories: Data-Engineering
feature-img: "assets/img/feature-img/pexels-danielspase-2091351.jpeg"
thumbnail: "assets/img/thumbnails/feature-img/pexels-danielspase-2091351.jpeg"
published: False
---

What are the best practice Spark and Delta features to optimize the performance of your Fabric Spark jobs?

This blog is _long_ overdue. It's only been the last year plus that I've 


| Feature                      | What it does                                                                                                                                                                                                   | Best Practice State                                                     | Session config applies to existing tables | Session Config                                                                                 | Runtime 1.3 | Runtime 2.0 |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- | --------------------------------------- | ---------------------------------------------------------------------------------------------- | ----------- | ----------- |
| Deletion Vectors             | Improves performance of MERGE, UPDATE, and DELETE via soft deleting invalidated records.                                                                                                                       | ENABLE                                                                  | No                                      |     spark.conf.set('spark.databricks.delta.properties.defaults.enableDeletionVectors', 'true') | UNSET       | ENABLED     |
| Auto Compaction              | Triggers compaction (OPTIMIZE) syncronously as part of write operations when too many small files exist.                                                                                                       | ENABLE                                                                  | Yes                                      |     spark.conf.set('spark.databricks.delta.autoCompact.enabled', 'true')                       | UNSET       | ENABLED     |
| Fast Optimize                | Minimizes performing suboptimal compaction jobs to improve the performance of OPTIMIZE and minimize write amplification.                                                                                       | ENABLE                                                                  | Yes                                     |     spark.conf.set('spark.microsoft.delta.optimize.fast.enabled', 'true')                      | DISABLED    | ENABLED     |
| Adaptive Target File Size    | Dynamically calculates the ideal target file size for data layout operations (OPTIMIZE, Auto Compact, and Optimize Write) to minimize write amplification, increase file skipping, and read/write parallelism. | ENABLE                                                                  | Yes                                     |     spark.conf.set('spark.microsoft.delta.targetFileSize.adaptive.enabled', 'true')            | DISABLED    | ENABLED     |
| File Level Compaction Target | Provides protection against write amplification when tables grow in size over time. Existing files that were considered compacted will not be recompacted when the target file size of the table increases.    | ENABLE                                                                  | Yes                                     |     spark.conf.set('spark.microsoft.delta.optimize.fileLevelTarget.enabled', 'true')           | DISABLED    | ENABLED     |
| Driver Mode Snapshot         | Improves performance of cold queries via processing Delta table snapshot on the driver node.                                                                                                                   | ENABLE                                                                  | Yes                                     |     spark.conf.set('spark.microsoft.delta.snapshot.driverMode.enabled', 'true')                | DISABLED    | ENABLED     |
| Parallel Snapshot Loading    | Improves performance of cold queries via processing Delta table snapshot in parallel.  | ENABLE                                                                  | Yes                                     |     spark.conf.set('spark.microsoft.delta.parallelSnapshotLoading.enabled', 'true')                | DISABLED    | ENABLED     |
| Optimize Write               | Shuffles small partitions of data together to target writing optimally sized files. | UNSET (enable on individual tables per detailed documentation)         | Yes                                     |     spark.conf.unset('spark.databricks.delta.optimizeWrite.enabled')                           | DISABLED    | UNSET       |
| V-Order                      | Applies VertiPaq style encoding and sorting to Parquet row groups to optimize the performance of Direct Lake Semantic Models.                                                                                  | UNSET (enable for tables directly used in Direct Lake Semantic Models) | Yes                                     |     spark.conf.unset('spark.sql.parquet.vorder.default')                                       | DISABLED    | UNSET       |